{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>concavity</th>\n",
       "      <th>concave points</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID diagnosis  radius  texture  perimeter    area  smoothness  \\\n",
       "0      842302         M   17.99    10.38     122.80  1001.0     0.11840   \n",
       "1      842517         M   20.57    17.77     132.90  1326.0     0.08474   \n",
       "2    84300903         M   19.69    21.25     130.00  1203.0     0.10960   \n",
       "3    84348301         M   11.42    20.38      77.58   386.1     0.14250   \n",
       "4    84358402         M   20.29    14.34     135.10  1297.0     0.10030   \n",
       "..        ...       ...     ...      ...        ...     ...         ...   \n",
       "564    926424         M   21.56    22.39     142.00  1479.0     0.11100   \n",
       "565    926682         M   20.13    28.25     131.20  1261.0     0.09780   \n",
       "566    926954         M   16.60    28.08     108.30   858.1     0.08455   \n",
       "567    927241         M   20.60    29.33     140.10  1265.0     0.11780   \n",
       "568     92751         B    7.76    24.54      47.92   181.0     0.05263   \n",
       "\n",
       "     compactness  concavity  concave points  ...   worst radius  \\\n",
       "0        0.27760    0.30010         0.14710  ...         25.380   \n",
       "1        0.07864    0.08690         0.07017  ...         24.990   \n",
       "2        0.15990    0.19740         0.12790  ...         23.570   \n",
       "3        0.28390    0.24140         0.10520  ...         14.910   \n",
       "4        0.13280    0.19800         0.10430  ...         22.540   \n",
       "..           ...        ...             ...  ...            ...   \n",
       "564      0.11590    0.24390         0.13890  ...         25.450   \n",
       "565      0.10340    0.14400         0.09791  ...         23.690   \n",
       "566      0.10230    0.09251         0.05302  ...         18.980   \n",
       "567      0.27700    0.35140         0.15200  ...         25.740   \n",
       "568      0.04362    0.00000         0.00000  ...          9.456   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "0            17.33           184.60      2019.0           0.16220   \n",
       "1            23.41           158.80      1956.0           0.12380   \n",
       "2            25.53           152.50      1709.0           0.14440   \n",
       "3            26.50            98.87       567.7           0.20980   \n",
       "4            16.67           152.20      1575.0           0.13740   \n",
       "..             ...              ...         ...               ...   \n",
       "564          26.40           166.10      2027.0           0.14100   \n",
       "565          38.25           155.00      1731.0           0.11660   \n",
       "566          34.12           126.70      1124.0           0.11390   \n",
       "567          39.42           184.60      1821.0           0.16500   \n",
       "568          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = ['ID','diagnosis','radius','texture','perimeter','area','smoothness','compactness','concavity','concave points','symmetry','fractal dimension','radius se','texture se','perimeter se','area se','smoothness se','compactness se','concavity se','concave points se','symmetry se','fractal dimension se',' worst radius','worst texture','worst perimeter','worst area','worst smoothness','worst compactness','worst concavity','worst concave points','worst symmetry','worst fractal dimension']\n",
    "df = pd.read_csv(\"/Users/ruthwetters/Documents/Neural Computing/wdbc.data\",names=colnames)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({\"diagnosis\":  {\"M\":1,\"B\":0}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = torch.from_numpy(df.drop('diagnosis', axis=1).to_numpy()).float()\n",
    "y = df['diagnosis'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cpu'\n",
    "mnist_dim = 31\n",
    "hidden_dim = 568\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=mnist_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.tanh(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=30,\n",
    "    lr=0.001,device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8195\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.7072\u001b[0m  0.0238\n",
      "      2        \u001b[36m0.7731\u001b[0m       0.6250        \u001b[35m0.6753\u001b[0m  0.0265\n",
      "      3        \u001b[36m0.7237\u001b[0m       0.6250        \u001b[35m0.6715\u001b[0m  0.0321\n",
      "      4        \u001b[36m0.7095\u001b[0m       0.6250        \u001b[35m0.6644\u001b[0m  0.0372\n",
      "      5        0.7510       0.6250        0.6650  0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.7016\u001b[0m       0.6250        0.6660  0.0703\n",
      "      7        0.7337       0.6250        0.6679  0.0622\n",
      "      8        0.7341       0.6250        \u001b[35m0.6644\u001b[0m  0.0479\n",
      "      9        0.7396       0.6250        0.6646  0.0377\n",
      "     10        0.7505       0.6250        0.6668  0.0444\n",
      "     11        0.7151       0.6250        0.6678  0.0404\n",
      "     12        0.7201       0.6250        0.6660  0.0405\n",
      "     13        0.7068       0.6250        0.6660  0.0388\n",
      "     14        0.7376       0.6250        0.6652  0.0368\n",
      "     15        0.7603       0.6250        0.6659  0.0507\n",
      "     16        0.7130       0.6250        0.6649  0.0502\n",
      "     17        0.7333       0.6250        \u001b[35m0.6640\u001b[0m  0.0493\n",
      "     18        0.7401       0.6250        0.6642  0.0436\n",
      "     19        0.7023       0.6250        0.6646  0.0372\n",
      "     20        0.7340       0.6250        0.6662  0.0466\n",
      "     21        0.7031       0.6250        0.6651  0.0317\n",
      "     22        0.7184       0.6250        0.6691  0.0421\n",
      "     23        0.7448       0.6250        0.6656  0.0458\n",
      "     24        0.7293       0.6250        0.6643  0.0510\n",
      "     25        0.7087       0.6250        0.6654  0.0631\n",
      "     26        0.7178       0.6250        \u001b[35m0.6639\u001b[0m  0.0435\n",
      "     27        0.7384       0.6250        0.6641  0.0468\n",
      "     28        0.7202       0.6250        0.6645  0.0623\n",
      "     29        0.7161       0.6250        0.6647  0.0685\n",
      "     30        \u001b[36m0.6904\u001b[0m       0.6250        0.6645  0.0562\n"
     ]
    }
   ],
   "source": [
    "model = net.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77       107\n",
      "           1       0.00      0.00      0.00        64\n",
      "\n",
      "    accuracy                           0.63       171\n",
      "   macro avg       0.31      0.50      0.38       171\n",
      "weighted avg       0.39      0.63      0.48       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the effect of early stopping\n",
    "from skorch.callbacks import EarlyStopping\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    lr=0.001,\n",
    "    max_epochs=100,\n",
    "    callbacks=[EarlyStopping()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7862\u001b[0m       \u001b[32m0.6125\u001b[0m        \u001b[35m0.6644\u001b[0m  0.0956\n",
      "      2        \u001b[36m0.7482\u001b[0m       \u001b[32m0.6250\u001b[0m        0.6656  0.0373\n",
      "      3        0.7558       0.6250        0.6684  0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.7104\u001b[0m       0.6250        0.6674  0.0296\n",
      "      5        0.7256       0.6250        0.6668  0.0523\n",
      "      6        0.7564       0.6250        \u001b[35m0.6629\u001b[0m  0.0512\n",
      "      7        \u001b[36m0.7018\u001b[0m       0.6250        0.6665  0.0507\n",
      "      8        \u001b[36m0.6681\u001b[0m       0.6250        0.6670  0.0409\n",
      "      9        0.7258       0.6250        0.6663  0.0419\n",
      "     10        0.7407       0.6250        0.6666  0.0400\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "net.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.9130\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m0.7324\u001b[0m  0.0257\n",
      "      2        \u001b[36m0.7669\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6769\u001b[0m  0.0367\n",
      "      3        0.8070       0.6250        \u001b[35m0.6634\u001b[0m  0.0311\n",
      "      4        \u001b[36m0.7119\u001b[0m       0.6250        \u001b[35m0.6618\u001b[0m  0.0355\n",
      "      5        \u001b[36m0.6840\u001b[0m       0.6250        \u001b[35m0.6618\u001b[0m  0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.6612\u001b[0m       0.6250        0.6618  0.0390\n",
      "      7        0.6799       0.6250        0.6621  0.0261\n",
      "      8        0.7675       0.6250        0.6628  0.0652\n",
      "      9        0.7005       0.6250        0.6625  0.0560\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7912\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6659\u001b[0m  0.0212\n",
      "      2        \u001b[36m0.7499\u001b[0m       0.6250        \u001b[35m0.6632\u001b[0m  0.0299\n",
      "      3        \u001b[36m0.7122\u001b[0m       0.6250        \u001b[35m0.6630\u001b[0m  0.0408\n",
      "      4        0.7135       0.6250        0.6637  0.0354\n",
      "      5        \u001b[36m0.6979\u001b[0m       0.6250        0.6648  0.0456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.7604       0.6250        0.6646  0.0345\n",
      "      7        0.7650       0.6250        0.6649  0.0451\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7158\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6633\u001b[0m  0.0366\n",
      "      2        0.7219       0.6250        0.6641  0.0386\n",
      "      3        0.7323       0.6250        0.6652  0.0395\n",
      "      4        0.7290       0.6250        0.6645  0.0349\n",
      "      5        0.7423       0.6250        0.6649  0.0315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8820\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m0.7275\u001b[0m  0.0681\n",
      "      2        \u001b[36m0.7844\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6762\u001b[0m  0.0291\n",
      "      3        \u001b[36m0.7003\u001b[0m       0.6250        \u001b[35m0.6637\u001b[0m  0.0340\n",
      "      4        0.7618       0.6250        \u001b[35m0.6616\u001b[0m  0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.7305       0.6250        0.6618  0.0811\n",
      "      6        0.7580       0.6250        0.6620  0.0311\n",
      "      7        0.7914       0.6250        0.6631  0.0297\n",
      "      8        0.7289       0.6250        0.6627  0.0387\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7185\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6648\u001b[0m  0.0191\n",
      "      2        0.7652       0.6250        \u001b[35m0.6635\u001b[0m  0.0250\n",
      "      3        0.7451       0.6250        \u001b[35m0.6628\u001b[0m  0.0272\n",
      "      4        0.7626       0.6250        \u001b[35m0.6616\u001b[0m  0.0325\n",
      "      5        0.7309       0.6250        0.6632  0.0275\n",
      "      6        \u001b[36m0.7081\u001b[0m       0.6250        0.6630  0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.7260       0.6250        0.6626  0.0345\n",
      "      8        0.7560       0.6250        0.6626  0.0281\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(net, X_train, y_train, scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the effect of weight decay\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    lr=0.001,\n",
    "    max_epochs=100,\n",
    "    callbacks=[EarlyStopping()],\n",
    "    optimizer__weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7842\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6699\u001b[0m  0.0712\n",
      "      2        \u001b[36m0.7642\u001b[0m       0.6250        \u001b[35m0.6635\u001b[0m  0.0382\n",
      "      3        \u001b[36m0.7402\u001b[0m       0.6250        0.6648  0.0487\n",
      "      4        \u001b[36m0.7134\u001b[0m       0.6250        0.6666  0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7119\u001b[0m       0.6250        0.6660  0.0611\n",
      "      6        0.7554       0.6250        0.6662  0.0458\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "net.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the effect of momentum\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    lr=0.001,\n",
    "    max_epochs=100,\n",
    "    callbacks=[EarlyStopping()],\n",
    "    optimizer__weight_decay=0.01,\n",
    "    optimizer__momentum=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.8322\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.6684\u001b[0m  0.0608\n",
      "      2        \u001b[36m0.7340\u001b[0m       0.6250        \u001b[35m0.6643\u001b[0m  0.0317\n",
      "      3        \u001b[36m0.7306\u001b[0m       0.6250        0.6658  0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.7110\u001b[0m       0.6250        0.6656  0.0463\n",
      "      5        0.7110       0.6250        0.6648  0.0674\n",
      "      6        0.7470       0.6250        0.6679  0.0401\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (hidden): Linear(in_features=31, out_features=568, bias=True)\n",
       "    (output): Linear(in_features=568, out_features=2, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.45988774, 0.33793783, 0.27283597, 0.46067595, 0.31627488]),\n",
       " 'score_time': array([0.00817513, 0.00849891, 0.01263809, 0.00759697, 0.01494598]),\n",
       " 'test_score': array([0.625     , 0.625     , 0.625     , 0.63291139, 0.63291139])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu', 'sigmoid', 'linear'],\n",
       "                         'alpha': [0.0001, 0.01, 0.05, 0.5],\n",
       "                         'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu','sigmoid','linear'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.01, 0.05, 0.5],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.623 (+/-0.013) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.623 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.623 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.623 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.631 (+/-0.010) for {'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.631 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.626 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.628 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.472 (+/-0.250) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.422 (+/-0.203) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.576 (+/-0.201) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.478 (+/-0.248) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.528 (+/-0.250) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.519 (+/-0.249) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.578 (+/-0.216) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.473 (+/-0.256) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.478 (+/-0.253) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.483 (+/-0.241) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.522 (+/-0.248) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.478 (+/-0.248) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.628 (+/-0.008) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.500 (+/-0.325) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.575 (+/-0.208) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.571 (+/-0.197) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.422 (+/-0.203) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.578 (+/-0.203) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.575 (+/-0.208) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.528 (+/-0.250) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.570 (+/-0.203) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.487 (+/-0.236) for {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.425 (+/-0.208) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.475 (+/-0.252) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.478 (+/-0.253) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.478 (+/-0.253) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.424 (+/-0.201) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.575 (+/-0.208) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.472 (+/-0.250) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.472 (+/-0.250) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.575 (+/-0.208) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.525 (+/-0.252) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.477 (+/-0.238) for {'activation': 'relu', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'sigmoid', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (50, 100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "nan (+/-nan) for {'activation': 'linear', 'alpha': 0.5, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best parameter set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_mlp.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "plot_roc_curve(model, X_test, y_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
